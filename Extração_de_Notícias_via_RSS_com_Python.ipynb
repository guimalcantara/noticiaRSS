{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM9mP8H6/K5h/eX1I5vKjBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guimalcantara/noticiaRSS/blob/main/Extra%C3%A7%C3%A3o_de_Not%C3%ADcias_via_RSS_com_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser beautifulsoup4\n"
      ],
      "metadata": {
        "id": "4rOoX6w0VVGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium webdriver-manager beautifulsoup4 requests python-dateutil\n"
      ],
      "metadata": {
        "id": "1edPUccFghd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil import parser as date_parser\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil import parser as date_parser\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIGURAÇÕES\n",
        "# -----------------------------------------------------------------------------\n",
        "FEED_URL    = \"https://about.instagram.com/pt-br/blog/announcements\"\n",
        "OUTPUT_FILE = Path(\"avisos_blog.json\")\n",
        "START_DATE  = datetime(2025, 1, 1)\n",
        "HEADERS     = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1) Parse do RSS\n",
        "# -----------------------------------------------------------------------------\n",
        "feed = feedparser.parse(FEED_URL)\n",
        "if feed.bozo:\n",
        "    raise RuntimeError(f\"Erro ao ler feed: {feed.bozo_exception}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2) Itera entradas e filtra por data\n",
        "# -----------------------------------------------------------------------------\n",
        "avisos = []\n",
        "for entry in feed.entries:\n",
        "    # published_parsed pode ser None em alguns feeds\n",
        "    pub_date = None\n",
        "    if hasattr(entry, \"published\"):\n",
        "        pub_date = date_parser.parse(entry.published)\n",
        "    elif hasattr(entry, \"updated\"):\n",
        "        pub_date = date_parser.parse(entry.updated)\n",
        "    if not pub_date or pub_date < START_DATE:\n",
        "        continue\n",
        "\n",
        "    url   = entry.link\n",
        "    title = entry.title\n",
        "    # resumo (HTML ou texto)\n",
        "    summary = getattr(entry, \"summary\", \"\")\n",
        "\n",
        "    # opcional: buscar conteúdo completo na página\n",
        "    try:\n",
        "        resp = requests.get(url, headers=HEADERS)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        body = soup.select_one(\".wysiwyg, .ContentBody\") or soup\n",
        "        paragraphs = [p.get_text(strip=True) for p in body.find_all(\"p\")]\n",
        "        full_content = \"\\n\\n\".join(paragraphs)\n",
        "    except Exception:\n",
        "        full_content = summary\n",
        "\n",
        "    avisos.append({\n",
        "        \"title\":   title,\n",
        "        \"url\":     url,\n",
        "        \"date\":    pub_date.isoformat(),\n",
        "        \"summary\": summary,\n",
        "        \"content\": full_content\n",
        "    })\n",
        "    print(f\"[OK] {title}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3) Grava JSON\n",
        "# -----------------------------------------------------------------------------\n",
        "with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(avisos, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nConcluído: {len(avisos)} avisos salvos em {OUTPUT_FILE}\")\n"
      ],
      "metadata": {
        "id": "O-KD2leVhsPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil import parser as date_parser\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIGURAÇÃO\n",
        "# -----------------------------------------------------------------------------\n",
        "BASE_URL     = \"https://about.instagram.com/pt-br/blog/announcements\"\n",
        "OUTPUT_PATH  = Path(\"avisos_announcements.json\")\n",
        "HEADERS      = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "HASH_TAG     = \"#AVISOS\"\n",
        "START_DATE   = datetime(2025, 1, 1)  # opcional\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1) Busca cada página numerada de “Announcements”\n",
        "# -----------------------------------------------------------------------------\n",
        "def fetch_announcements_page(page: int) -> BeautifulSoup | None:\n",
        "    url = BASE_URL if page == 1 else f\"{BASE_URL}/page/{page}\"\n",
        "    resp = requests.get(url, headers=HEADERS)\n",
        "    if resp.status_code != 200:\n",
        "        return None\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    # busca cartão de anúncio\n",
        "    cards = soup.select(\"article, .ContentItem\")\n",
        "    return soup if cards else None\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2) Filtra cartões que contenham #AVISOS e extrai suas URLs\n",
        "# -----------------------------------------------------------------------------\n",
        "def extract_avisos_urls(soup: BeautifulSoup) -> list[str]:\n",
        "    urls = []\n",
        "    for card in soup.select(\"article, .ContentItem\"):\n",
        "        text = card.get_text(\" \", strip=True).upper()\n",
        "        if HASH_TAG not in text:\n",
        "            continue\n",
        "        link = card.find(\"a\", href=True)\n",
        "        if link:\n",
        "            urls.append(link[\"href\"].strip())\n",
        "    # remove duplicatas mantendo ordem\n",
        "    seen, unique = set(), []\n",
        "    for u in urls:\n",
        "        if u not in seen:\n",
        "            unique.append(u); seen.add(u)\n",
        "    return unique\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3) Para cada URL, faz download e extrai título, data e corpo completo\n",
        "# -----------------------------------------------------------------------------\n",
        "def fetch_full_post(url: str) -> dict | None:\n",
        "    resp = requests.get(url, headers=HEADERS)\n",
        "    resp.raise_for_status()\n",
        "    p = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # título\n",
        "    h = p.find([\"h1\", \"h2\"])\n",
        "    title = h.get_text(strip=True) if h else \"\"\n",
        "\n",
        "    # data\n",
        "    t = p.find(\"time\")\n",
        "    date_iso = t.get(\"datetime\", \"\") if t else \"\"\n",
        "    # opcional: filtrar por data mínima\n",
        "    if date_iso:\n",
        "        try:\n",
        "            dt = date_parser.parse(date_iso, dayfirst=True)\n",
        "            if dt < START_DATE:\n",
        "                return None\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # corpo\n",
        "    body = p.select_one(\".wysiwyg, .ContentBody\") or p\n",
        "    paras = [pt.get_text(strip=True) for pt in body.find_all(\"p\")]\n",
        "    content = \"\\n\\n\".join(paras)\n",
        "\n",
        "    return {\"url\": url, \"title\": title, \"date\": date_iso, \"content\": content}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4) Fluxo principal\n",
        "# -----------------------------------------------------------------------------\n",
        "def main():\n",
        "    page     = 1\n",
        "    all_urls = []\n",
        "\n",
        "    # percorre páginas até não encontrar novos cartões\n",
        "    while True:\n",
        "        soup = fetch_announcements_page(page)\n",
        "        if soup is None:\n",
        "            break\n",
        "        urls = extract_avisos_urls(soup)\n",
        "        if not urls:\n",
        "            break\n",
        "        all_urls.extend(urls)\n",
        "        page += 1\n",
        "\n",
        "    # extrai conteúdo completo\n",
        "    avisos = []\n",
        "    for u in all_urls:\n",
        "        try:\n",
        "            post = fetch_full_post(u)\n",
        "            if post:\n",
        "                avisos.append(post)\n",
        "                print(f\"[OK] {post['title']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERRO] {u} → {e}\")\n",
        "\n",
        "    # grava JSON\n",
        "    with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(avisos, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\nConcluído: {len(avisos)} avisos salvos em {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cNUP_0c9i_9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Abaixo segue o código solicitado, com explicações detalhadas em português para uso em ambiente Jupyter/Colab\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def fetch_sitemap_urls(sitemap_url):\n",
        "    \"\"\"\n",
        "    Recupera URLs do sitemap que apontem para a seção 'announcements' em português.\n",
        "    Retorna lista de tuplas (url, lastmod).\n",
        "    \"\"\"\n",
        "    resp = requests.get(sitemap_url)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.content, \"xml\")\n",
        "\n",
        "    items = []\n",
        "    for url in soup.find_all(\"url\"):\n",
        "        loc = url.loc.text\n",
        "        if \"/pt-br/blog/announcements/\" in loc:\n",
        "            lastmod = url.lastmod.text\n",
        "            pub_dt = datetime.fromisoformat(lastmod)\n",
        "            items.append((loc, pub_dt))\n",
        "    return items\n",
        "\n",
        "def fetch_full_content(url):\n",
        "    \"\"\"\n",
        "    Faz requisição à página e extrai conteúdo principal via <article> ou div com class 'blog-post-content'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "        article = soup.find(\"article\") or soup.find(\"div\", class_=\"blog-post-content\")\n",
        "        return article.get_text(separator=\"\\n\").strip() if article else \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    # 1) URL do sitemap oficial de anúncios\n",
        "    sitemap_url = \"https://about.instagram.com/pt-br/announcements/sitemap.xml\"\n",
        "\n",
        "    # 2) Recupera URLs e datas\n",
        "    entries = fetch_sitemap_urls(sitemap_url)\n",
        "\n",
        "    data = []\n",
        "    for url, pub_dt in tqdm(entries, desc=\"Baixando anúncios\"):\n",
        "        conteudo = fetch_full_content(url)\n",
        "\n",
        "        # Extração segura do título\n",
        "        page_soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
        "        title_element = page_soup.find(\"h1\")\n",
        "        title = title_element.get_text(strip=True) if title_element else \"\"\n",
        "\n",
        "        data.append({\n",
        "            \"titulo\": title,\n",
        "            \"link\": url,\n",
        "            \"data_publicacao\": pub_dt,\n",
        "            \"conteudo_completo\": conteudo  # aqui agora vai todo o texto\n",
        "        })\n",
        "\n",
        "    # 3) Criar DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Execução\n",
        "df_announcements = main()\n",
        "print(df_announcements.head())\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RynxieVZXViZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Abaixo segue o código solicitado, com explicações detalhadas em português para uso em ambiente Jupyter/Colab\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def fetch_sitemap_urls(sitemap_url):\n",
        "    \"\"\"\n",
        "    Recupera URLs do sitemap que apontem para a seção 'announcements' em português.\n",
        "    Retorna lista de tuplas (url, lastmod).\n",
        "    \"\"\"\n",
        "    resp = requests.get(sitemap_url)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.content, \"xml\")\n",
        "\n",
        "    items = []\n",
        "    for url in soup.find_all(\"url\"):\n",
        "        loc = url.loc.text\n",
        "        if \"/pt-br/blog/announcements/\" in loc:\n",
        "            lastmod = url.lastmod.text\n",
        "            pub_dt = datetime.fromisoformat(lastmod)\n",
        "            items.append((loc, pub_dt))\n",
        "    return items\n",
        "\n",
        "def fetch_all_text_elements(soup):\n",
        "    \"\"\"\n",
        "    Extrai texto de todos os elementos relevantes da página para montar conteúdo completo.\n",
        "    \"\"\"\n",
        "    sections = []\n",
        "\n",
        "    # Título principal\n",
        "    titulo = soup.find(\"h1\")\n",
        "    if titulo:\n",
        "        sections.append(f\"# {titulo.get_text(strip=True)}\")\n",
        "\n",
        "    # Subtítulos e seções\n",
        "    for tag in soup.find_all([\"h2\", \"h3\", \"h4\", \"p\", \"ul\", \"ol\"]):\n",
        "        text = tag.get_text(separator=\" \", strip=True)\n",
        "        if text:\n",
        "            sections.append(text)\n",
        "\n",
        "    return \"\\n\\n\".join(sections).strip()\n",
        "\n",
        "def fetch_full_page_content(url):\n",
        "    \"\"\"\n",
        "    Recupera o HTML da página e extrai o título e todo o conteúdo útil.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "        titulo_tag = soup.find(\"h1\")\n",
        "        titulo = titulo_tag.get_text(strip=True) if titulo_tag else \"(sem título)\"\n",
        "\n",
        "        conteudo_completo = fetch_all_text_elements(soup)\n",
        "\n",
        "        return titulo, conteudo_completo\n",
        "    except:\n",
        "        return \"(erro ao obter título)\", \"\"\n",
        "\n",
        "def main():\n",
        "    sitemap_url = \"https://about.instagram.com/pt-br/announcements/sitemap.xml\"\n",
        "    entries = fetch_sitemap_urls(sitemap_url)\n",
        "\n",
        "    data = []\n",
        "    for url, pub_dt in tqdm(entries, desc=\"Baixando e extraindo dados\"):\n",
        "        titulo, conteudo = fetch_full_page_content(url)\n",
        "        data.append({\n",
        "            \"titulo\": titulo,\n",
        "            \"link\": url,\n",
        "            \"data_publicacao\": pub_dt,\n",
        "            \"conteudo_completo\": conteudo\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Execução\n",
        "df_anuncios_completos = main()\n",
        "df_anuncios_completos.head()\n"
      ],
      "metadata": {
        "id": "cQ-byPK3YduS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime, date\n",
        "import time\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook para ambiente Jupyter/Colab\n",
        "\n",
        "# --- Configurações ---\n",
        "BASE_URL = \"https://about.instagram.com\"\n",
        "ANNOUNCEMENTS_PATH = \"/pt-br/blog/announcements\"\n",
        "FULL_LISTING_URL = BASE_URL + ANNOUNCEMENTS_PATH\n",
        "\n",
        "# Definir a data de início para filtrar (início do ano atual)\n",
        "START_DATE = date(datetime.now().year, 1, 1)\n",
        "\n",
        "# Headers para simular um navegador (pode ajudar a evitar bloqueios simples)\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# --- Funções Auxiliares ---\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Tenta parsear a string de data 'Month Day, Year' para um objeto date.\"\"\"\n",
        "    try:\n",
        "        # Ex: \"February 22, 2024\"\n",
        "        # O local 'pt_BR' pode ser necessário dependendo do sistema, mas o formato %B (nome completo do mês)\n",
        "        # geralmente funciona com nomes de mês em inglês por padrão em muitos ambientes Python.\n",
        "        # Se der erro de localidade, pode precisar configurar.\n",
        "        # import locale\n",
        "        # locale.setlocale(locale.LC_TIME, 'pt_BR.UTF-8') # Pode variar dependendo do OS\n",
        "        return datetime.strptime(date_str, \"%B %d, %Y\").date()\n",
        "    except ValueError:\n",
        "        print(f\"Erro ao parsear data: {date_str}\")\n",
        "        return None\n",
        "\n",
        "def get_page_content(url):\n",
        "    \"\"\"Faz a requisição GET para uma URL e retorna o objeto BeautifulSoup.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10) # Timeout de 10 segundos\n",
        "        response.raise_for_status() # Lança exceção para status de erro (4xx ou 5xx)\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro ao buscar a página {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_article_links_and_dates(soup):\n",
        "    \"\"\"Extrai URLs, títulos e datas da página de listagem.\"\"\"\n",
        "    articles_list = []\n",
        "    # Seletor baseado na inspeção manual da página (PODE MUDAR!)\n",
        "    # Container principal dos artigos\n",
        "    articles_container = soup.find('div', class_='_aajr')\n",
        "\n",
        "    if not articles_container:\n",
        "        print(\"Não foi possível encontrar o container principal dos artigos. Verifique os seletores.\")\n",
        "        return []\n",
        "\n",
        "    # Itens individuais de artigo\n",
        "    article_items = articles_container.find_all('div', class_='_aajs')\n",
        "\n",
        "    if not article_items:\n",
        "        print(\"Não foi possível encontrar itens de artigos. Verifique os seletores.\")\n",
        "        return []\n",
        "\n",
        "    for item in article_items:\n",
        "        link_tag = item.find('a', href=True)\n",
        "        date_tag = item.find('div', class_='_aajt') # Seletor para a data (PODE MUDAR!)\n",
        "\n",
        "        if link_tag and date_tag:\n",
        "            relative_url = link_tag['href']\n",
        "            full_url = BASE_URL + relative_url\n",
        "            title = link_tag.get_text(strip=True)\n",
        "            date_str = date_tag.get_text(strip=True)\n",
        "            article_date = parse_date(date_str)\n",
        "\n",
        "            if article_date:\n",
        "                articles_list.append({\n",
        "                    'title': title,\n",
        "                    'url': full_url,\n",
        "                    'date': article_date\n",
        "                })\n",
        "        else:\n",
        "             print(f\"Aviso: Item de artigo encontrado sem link ou data esperados: {item.prettify()[:200]}...\")\n",
        "\n",
        "\n",
        "    return articles_list\n",
        "\n",
        "def extract_article_content(soup):\n",
        "    \"\"\"Extrai o texto completo do corpo de um artigo individual.\"\"\"\n",
        "    # Seletor para o container principal do conteúdo (PODE MUDAR!)\n",
        "    content_container = soup.find('div', class_='_aajk _aajp') # Exemplo de seletores combinados\n",
        "\n",
        "    if not content_container:\n",
        "        # Tentar encontrar o título caso o container principal falhe\n",
        "        title_tag = soup.find('h1') or soup.find('h2')\n",
        "        title_text = title_tag.get_text(strip=True) if title_tag else \"Título não encontrado\"\n",
        "        print(f\"Não foi possível encontrar o container de conteúdo para o artigo '{title_text}'. Verifique os seletores.\")\n",
        "        # Retorna texto do corpo inteiro da página (menos ideal) ou uma mensagem de erro\n",
        "        # return soup.get_text(strip=True) # Ou retornar o texto bruto da página inteira\n",
        "        return \"Conteúdo principal não encontrado com os seletores definidos.\"\n",
        "\n",
        "    # Extrair texto de todos os parágrafos dentro do container\n",
        "    paragraphs = content_container.find_all('p')\n",
        "    if paragraphs:\n",
        "        # Juntar o texto dos parágrafos, adicionando quebras de linha\n",
        "        full_text = \"\\n\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "    else:\n",
        "        # Se não encontrar parágrafos, tenta pegar todo o texto do container principal\n",
        "        full_text = content_container.get_text(strip=True)\n",
        "        if not full_text:\n",
        "             full_text = \"Nenhum texto de parágrafo ou conteúdo encontrado no container.\"\n",
        "\n",
        "\n",
        "    return full_text\n",
        "\n",
        "# --- Processo Principal ---\n",
        "\n",
        "print(f\"Buscando a lista de artigos em: {FULL_LISTING_URL}\")\n",
        "listing_soup = get_page_content(FULL_LISTING_URL)\n",
        "\n",
        "if listing_soup:\n",
        "    all_articles_info = extract_article_links_and_dates(listing_soup)\n",
        "\n",
        "    # Filtrar artigos pela data (desde o início do ano)\n",
        "    articles_this_year = [\n",
        "        art for art in all_articles_info if art['date'] >= START_DATE\n",
        "    ]\n",
        "\n",
        "    print(f\"Encontrados {len(all_articles_info)} artigos listados na página.\")\n",
        "    print(f\"Filtrando para artigos desde {START_DATE.strftime('%d/%m/%Y')}.\")\n",
        "    print(f\"Serão processados {len(articles_this_year)} artigos que atendem ao filtro de data.\")\n",
        "\n",
        "    extracted_data = []\n",
        "\n",
        "    # Usar tqdm para mostrar o progresso ao processar cada artigo\n",
        "    print(\"\\nIniciando extração do conteúdo completo dos artigos...\")\n",
        "    for article_info in tqdm(articles_this_year, desc=\"Processando artigos\"):\n",
        "        article_url = article_info['url']\n",
        "        article_title = article_info['title']\n",
        "        article_date = article_info['date']\n",
        "\n",
        "        article_soup = get_page_content(article_url)\n",
        "\n",
        "        if article_soup:\n",
        "            full_content = extract_article_content(article_soup)\n",
        "\n",
        "            extracted_data.append({\n",
        "                'Título': article_title,\n",
        "                'URL': article_url,\n",
        "                'Data': article_date,\n",
        "                'Conteúdo Completo': full_content\n",
        "            })\n",
        "        else:\n",
        "            extracted_data.append({\n",
        "                 'Título': article_title,\n",
        "                 'URL': article_url,\n",
        "                 'Data': article_date,\n",
        "                 'Conteúdo Completo': \"Erro ao carregar ou parsear a página do artigo.\"\n",
        "            })\n",
        "\n",
        "        # Pequeno delay para ser educado\n",
        "        time.sleep(0.5) # Atraso de meio segundo entre as requisições\n",
        "\n",
        "    # Criar o DataFrame pandas\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "\n",
        "    # Opcional: Ordenar por data\n",
        "    df = df.sort_values(by='Data', ascending=False).reset_index(why='drop')\n",
        "\n",
        "    print(\"\\nExtração concluída.\")\n",
        "    print(\"DataFrame criado:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nTotal de artigos extraídos: {len(df)}\")\n",
        "\n",
        "else:\n",
        "    print(\"Não foi possível buscar a página de listagem. Encerrando o script.\")"
      ],
      "metadata": {
        "id": "f7vvYbWvaVz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime, date\n",
        "import time\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook para ambiente Jupyter/Colab\n",
        "\n",
        "# --- Configurações ---\n",
        "BASE_URL = \"https://about.instagram.com\"\n",
        "ANNOUNCEMENTS_PATH = \"/pt-br/blog/announcements\"\n",
        "FULL_LISTING_URL = BASE_URL + ANNOUNCEMENTS_PATH\n",
        "\n",
        "# Definir a data de início para filtrar (início do ano atual)\n",
        "START_DATE = date(datetime.now().year, 1, 1)\n",
        "\n",
        "# Headers para simular um navegador (pode ajudar a evitar bloqueios simples)\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Configurações para retentativa (retry)\n",
        "MAX_RETRIES = 5\n",
        "RETRY_DELAY_SECONDS = 5 # Tempo de espera inicial para retentativa 429\n",
        "\n",
        "# --- Funções Auxiliares ---\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Tenta parsear a string de data 'Month Day, Year' para um objeto date.\"\"\"\n",
        "    try:\n",
        "        # Ex: \"February 22, 2024\"\n",
        "        return datetime.strptime(date_str, \"%B %d, %Y\").date()\n",
        "    except ValueError:\n",
        "        print(f\"Erro ao parsear data: {date_str}\")\n",
        "        return None\n",
        "\n",
        "def get_page_content(url, retries=0):\n",
        "    \"\"\"\n",
        "    Faz a requisição GET para uma URL, retorna o objeto BeautifulSoup.\n",
        "    Inclui retentativa para erro 429.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Buscando: {url}\") # Indica qual URL está sendo buscada\n",
        "        response = requests.get(url, headers=HEADERS, timeout=15) # Aumentar timeout\n",
        "        response.raise_for_status() # Lança exceção para status de erro (4xx ou 5xx) exceto 429 tratado abaixo\n",
        "\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if e.response.status_code == 429:\n",
        "            if retries < MAX_RETRIES:\n",
        "                wait_time = RETRY_DELAY_SECONDS * (2 ** retries) # Espera exponencial\n",
        "                print(f\"Erro 429 (Too Many Requests) para {url}. Tentando novamente em {wait_time} segundos (Tentativa {retries + 1}/{MAX_RETRIES})...\")\n",
        "                time.sleep(wait_time)\n",
        "                return get_page_content(url, retries + 1) # Chama a função recursivamente\n",
        "            else:\n",
        "                print(f\"Erro 429 persistente para {url} após {MAX_RETRIES} tentativas. Pulando...\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"Erro HTTP {e.response.status_code} ao buscar a página {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro ao buscar a página {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_article_links_and_dates(soup):\n",
        "    \"\"\"Extrai URLs, títulos e datas da página de listagem.\"\"\"\n",
        "    articles_list = []\n",
        "    # Seletor baseado na inspeção manual da página (PODE MUDAR!)\n",
        "    # Container principal dos artigos\n",
        "    articles_container = soup.find('div', class_='_aajr')\n",
        "\n",
        "    if not articles_container:\n",
        "        print(\"Não foi possível encontrar o container principal dos artigos. Verifique os seletores (_aajr).\")\n",
        "        return []\n",
        "\n",
        "    # Itens individuais de artigo\n",
        "    article_items = articles_container.find_all('div', class_='_aajs')\n",
        "\n",
        "    if not article_items:\n",
        "        print(\"Não foi possível encontrar itens de artigos. Verifique os seletores (_aajs).\")\n",
        "        return []\n",
        "\n",
        "    for item in article_items:\n",
        "        link_tag = item.find('a', href=True)\n",
        "        date_tag = item.find('div', class_='_aajt') # Seletor para a data (PODE MUDAR!)\n",
        "\n",
        "        if link_tag and date_tag:\n",
        "            relative_url = link_tag['href']\n",
        "            full_url = BASE_URL + relative_url\n",
        "            title = link_tag.get_text(strip=True)\n",
        "            date_str = date_tag.get_text(strip=True)\n",
        "            article_date = parse_date(date_str)\n",
        "\n",
        "            if article_date:\n",
        "                articles_list.append({\n",
        "                    'title': title,\n",
        "                    'url': full_url,\n",
        "                    'date': article_date\n",
        "                })\n",
        "            else:\n",
        "                 print(f\"Aviso: Data não parseada para o item: {date_str}. Pulando item.\")\n",
        "        else:\n",
        "             print(f\"Aviso: Item de artigo encontrado sem link ou data esperados. Pulando item.\")\n",
        "\n",
        "\n",
        "    return articles_list\n",
        "\n",
        "def extract_article_content(soup):\n",
        "    \"\"\"Extrai o texto completo do corpo de um artigo individual.\"\"\"\n",
        "    # Seletor para o container principal do conteúdo (PODE MUDAR!)\n",
        "    # Pode haver múltiplos containers dependendo do layout da página.\n",
        "    # Vamos tentar encontrar o container com a classe específica,\n",
        "    # mas se falhar, talvez pegar o texto de todos os parágrafos dentro de uma área mais geral.\n",
        "    content_container = soup.find('div', class_='_aajk _aajp') # Exemplo de seletores combinados\n",
        "\n",
        "    if not content_container:\n",
        "        # Tentar encontrar o título para o print de erro\n",
        "        title_tag = soup.find('h1') or soup.find('h2')\n",
        "        title_text = title_tag.get_text(strip=True) if title_tag else \"Título não encontrado\"\n",
        "        print(f\"Aviso: Não foi possível encontrar o container de conteúdo principal para o artigo '{title_text}'. Verifique os seletores (_aajk _aajp). Tentando extrair parágrafos de uma área mais ampla.\")\n",
        "\n",
        "        # Tentar encontrar parágrafos em uma área potencialmente mais estável\n",
        "        # Pode ser necessário ajustar este seletor também\n",
        "        paragraphs = soup.select('div._a8z9 p, div._aajk p') # Tenta parágrafos dentro de _a8z9 ou _aajk\n",
        "        if paragraphs:\n",
        "             full_text = \"\\n\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "             if full_text:\n",
        "                 return full_text\n",
        "             else:\n",
        "                 return \"Conteúdo principal não encontrado com os seletores definidos, mesmo em áreas amplas.\"\n",
        "        else:\n",
        "             return \"Nenhum parágrafo de conteúdo encontrado com os seletores definidos.\"\n",
        "\n",
        "\n",
        "    # Extrair texto de todos os parágrafos dentro do container específico\n",
        "    paragraphs = content_container.find_all('p')\n",
        "    if paragraphs:\n",
        "        # Juntar o texto dos parágrafos, adicionando quebras de linha\n",
        "        full_text = \"\\n\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "    else:\n",
        "        # Se não encontrar parágrafos no container específico, tenta pegar todo o texto dele\n",
        "        full_text = content_container.get_text(strip=True)\n",
        "        if not full_text:\n",
        "             full_text = \"Nenhum texto de parágrafo ou conteúdo encontrado no container específico.\"\n",
        "\n",
        "    return full_text\n",
        "\n",
        "\n",
        "# --- Processo Principal ---\n",
        "\n",
        "print(f\"Iniciando busca pela lista de artigos em: {FULL_LISTING_URL}\")\n",
        "\n",
        "# Adiciona um pequeno delay antes da primeira requisição também\n",
        "time.sleep(2) # Espera 2 segundos antes de começar\n",
        "\n",
        "listing_soup = get_page_content(FULL_LISTING_URL)\n",
        "\n",
        "if listing_soup:\n",
        "    all_articles_info = extract_article_links_and_dates(listing_soup)\n",
        "\n",
        "    # Filtrar artigos pela data (desde o início do ano)\n",
        "    articles_this_year = [\n",
        "        art for art in all_articles_info if art['date'] >= START_DATE\n",
        "    ]\n",
        "\n",
        "    print(f\"Encontrados {len(all_articles_info)} artigos listados na página (podem ser mais do que o filtro de data).\")\n",
        "    print(f\"Filtrando para artigos desde {START_DATE.strftime('%d/%m/%Y')}.\")\n",
        "    print(f\"Serão processados {len(articles_this_year)} artigos que atendem ao filtro de data.\")\n",
        "\n",
        "    extracted_data = []\n",
        "\n",
        "    # Usar tqdm para mostrar o progresso ao processar cada artigo\n",
        "    print(\"\\nIniciando extração do conteúdo completo dos artigos...\")\n",
        "    # Adiciona um pequeno delay antes de começar a iterar sobre os artigos também\n",
        "    time.sleep(1)\n",
        "\n",
        "    for article_info in tqdm(articles_this_year, desc=\"Processando artigos\"):\n",
        "        article_url = article_info['url']\n",
        "        article_title = article_info['title']\n",
        "        article_date = article_info['date']\n",
        "\n",
        "        article_soup = get_page_content(article_url) # get_page_content já tem retry/delay para 429\n",
        "\n",
        "        if article_soup:\n",
        "            full_content = extract_article_content(article_soup)\n",
        "\n",
        "            extracted_data.append({\n",
        "                'Título': article_title,\n",
        "                'URL': article_url,\n",
        "                'Data': article_date,\n",
        "                'Conteúdo Completo': full_content\n",
        "            })\n",
        "        else:\n",
        "            extracted_data.append({\n",
        "                 'Título': article_title,\n",
        "                 'URL': article_url,\n",
        "                 'Data': article_date,\n",
        "                 'Conteúdo Completo': \"Erro ao carregar ou parsear a página do artigo individual após retentativas.\"\n",
        "            })\n",
        "\n",
        "        # Pequeno delay para ser educado (além da retentativa no get_page_content)\n",
        "        # Isso garante um tempo mínimo entre as requisições de artigos diferentes.\n",
        "        time.sleep(1) # Atraso de 1 segundo entre processar cada artigo\n",
        "\n",
        "    # Criar o DataFrame pandas\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "\n",
        "    # Opcional: Ordenar por data\n",
        "    df = df.sort_values(by='Data', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nExtração concluída.\")\n",
        "    print(\"DataFrame criado:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nTotal de artigos extraídos: {len(df)}\")\n",
        "\n",
        "else:\n",
        "    print(\"Não foi possível buscar a página de listagem após retentativas. Encerrando o script.\")"
      ],
      "metadata": {
        "id": "aKcS7pxNcW2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL da página de anúncios do blog do Instagram\n",
        "url = 'https://about.instagram.com/pt-br/blog/announcements'\n",
        "\n",
        "# Faz a requisição HTTP\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Lista para armazenar resultados\n",
        "resultados = []\n",
        "\n",
        "# A estrutura real da página pode conter blocos de artigos em divs específicas\n",
        "# Aqui buscamos todos os links de artigos com base em observações práticas\n",
        "for artigo in soup.find_all('a', href=True):\n",
        "    href = artigo['href']\n",
        "    texto = artigo.get_text(strip=True)\n",
        "\n",
        "    # Filtro: considerar apenas links que levam para posts e têm título visível\n",
        "    if texto and href.startswith('/pt-br/blog/') and 'announcements' not in href:\n",
        "        resultados.append((texto, f\"https://about.instagram.com{href}\"))\n",
        "\n",
        "# Salva em arquivo de texto\n",
        "with open('conteudo_instagram.txt', 'w', encoding='utf-8') as arquivo:\n",
        "    for titulo, link in resultados:\n",
        "        arquivo.write(f\"Título: {titulo}\\n\")\n",
        "        arquivo.write(f\"Link: {link}\\n\\n\")\n"
      ],
      "metadata": {
        "id": "hRNUXI4yfWW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}